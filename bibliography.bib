@article{Imbens2015,
author = {Imbens, Guido W. and Rubin, Donald B.},
doi = {10.1017/cbo9781139025751.013},
file = {:Users/nrindtor/Documents/papers{\_}pile/unconfounded{\_}treatment{\_}assignment.pdf:pdf},
isbn = {9781139025751},
journal = {Causal Inference for Statistics, Social, and Biomedical Sciences},
pages = {257--280},
title = {{Unconfounded Treatment Assignment}},
year = {2015}
}
@article{Gershman2018,
abstract = {Randomized clinical trials are considered the preferred approach for comparing the effects of treatments, yet data from high-quality clinical trials are often unavailable and many clinical decisions are made on the basis of evidence from observational studies. Using clinical examples about the management of infertility, we discuss how we can use observational data from large and information-rich health-care databases combined with modern epidemiological and statistical methods to learn about the effects of interventions when clinical trial evidence is unavailable or not applicable to the clinically relevant target population. When trial evidence is unavailable, we can conduct observational analyses emulating the hypothetical pragmatic target trials that would address the clinical questions of interest. When trial evidence is available but not applicable to the clinically relevant target population, we can transport inferences from trial participants to the target population using the trial data and a sample of observational data from the target population. Clinical trial emulations and transportability analyses can be coupled with methods for examining heterogeneity of treatment effects, providing a path toward personalized medicine.},
author = {Gershman, Boris and Guo, David P. and Dahabreh, Issa J.},
doi = {10.1016/j.fertnstert.2018.04.005},
file = {:Users/nrindtor/Documents/papers{\_}pile/1-s2.0-S0015028218303303-main.pdf:pdf},
issn = {15565653},
journal = {Fertility and Sterility},
keywords = {Causal inference,emulating clinical trials using observational data,heterogeneity of treatment effects,study design,transportability},
number = {6},
pages = {946--951},
pmid = {29935652},
publisher = {Elsevier Inc.},
title = {{Using observational data for personalized medicine when clinical trial evidence is limited}},
url = {https://doi.org/10.1016/j.fertnstert.2018.04.005},
volume = {109},
year = {2018}
}
@book{P.Murphy2012,
archivePrefix = {arXiv},
arxivId = {0-387-31073-8},
author = {{P. Murphy}, Kevin},
booktitle = {Chance encounters: Probability in {\ldots}},
doi = {10.1038/217994a0},
eprint = {0-387-31073-8},
file = {:Users/nrindtor/Documents/papers{\_}pile/Machine Learning{\_} A Probabilistic Perspective [Murphy 2012-08-24].pdf:pdf},
isbn = {9780262018029},
issn = {10495258},
pmid = {20236947},
title = {{A probabilistic perspective}},
url = {http://link.springer.com/chapter/10.1007/978-94-011-3532-0{\_}2},
year = {2012}
}
@book{Shanmugam2018,
abstract = {"The mathematization of causality is a relatively recent development, and has become increasingly important in data science and machine learning. This book offers a self-contained and concise introduction to causal models and how to learn them from data"--Back of book.},
author = {Shanmugam, Ramalingam},
booktitle = {Journal of Statistical Computation and Simulation},
doi = {10.1080/00949655.2018.1505197},
file = {:Users/nrindtor/Documents/papers{\_}pile/11283.pdf:pdf},
isbn = {9780262037310},
issn = {0094-9655},
number = {16},
pages = {3248--3248},
title = {{Elements of causal inference: foundations and learning algorithms}},
volume = {88},
year = {2018}
}
@article{Lyons2018,
abstract = {causal; confounding; counterfactuals; effects; graphical methods; potential-outcome; structural equation models},
author = {Lyons, Jack and Ward, Barry},
doi = {10.4324/9781315542287-6},
file = {:Users/nrindtor/Documents/papers{\_}pile/hernanrobins{\_}v1.10.38.pdf:pdf},
isbn = {9780873387064},
journal = {The New Critical Thinking},
pages = {157--186},
title = {{Causal Inference}},
year = {2018}
}
@book{RasmussenC.EadnWilliams2004,
abstract = {Gaussian processes (GPs) are natural generalisations of multivariate Gaussian random variables to infinite (countably or continuous) index sets. GPs have been applied in a large number of fields to a diverse range of ends, and very many deep theoretical analyses of various properties are available. This paper gives an introduction to Gaussian processes on a fairly elementary level with special emphasis on characteristics relevant in machine learning. It draws explicit connections to branches such as spline smoothing models and support vector machines in which similar ideas have been investigated. Gaussian process models are routinely used to solve hard machine learning problems. They are attractive because of their flexible non-parametric nature and computational simplicity. Treated within a Bayesian framework, very powerful statistical methods can be implemented which offer valid estimates of uncertainties in our predictions and generic model selection procedures cast as nonlinear optimization problems. Their main drawback of heavy computational scaling has recently been alleviated by the introduction of generic sparse approximations.13,78,31 The mathematical literature on GPs is large and often uses deep concepts which are not required to fully understand most machine learning applications. In this tutorial paper, we aim to present characteristics of GPs relevant to machine learning and to show up precise connections to other "kernel machines" popular in the community. Our focus is on a simple presentation, but references to more detailed sources are provided.},
archivePrefix = {arXiv},
arxivId = {026218253X},
author = {{Rasmussen, C.E adn Williams}, C.K.I},
booktitle = {International journal of neural systems},
doi = {10.1142/S0129065704001899},
eprint = {026218253X},
file = {:Users/nrindtor/Documents/papers{\_}pile/RW.pdf:pdf},
isbn = {026218253X},
issn = {0129-0657},
number = {2},
pages = {69--106},
pmid = {15112367},
title = {{Gaussian processes for machine learning.}},
volume = {14},
year = {2004}
}
@article{Fellingham2011,
author = {Fellingham, Gilbert W},
file = {:Users/nrindtor/Documents/papers{\_}pile/BMI 715 - lecture 13 - 12.06.18 - Introduction to Bayesian methods.pdf:pdf},
title = {{Introduction to Bayesian Methods}},
year = {2011}
}
@article{Miao2018,
abstract = {We consider a causal effect that is confounded by an unobserved variable, but with observed proxy variables of the confounder. We show that, with at least two independent proxy variables satisfying a certain rank condition, the causal effect is nonparametrically identified, even if the measurement error mechanism, i.e., the conditional distribution of the proxies given the con- founder, may not be identified. Our result generalizes the identification strategy of Kuroki {\&} Pearl (2014) that rests on identification of the measurement error mechanism. When only one proxy for the confounder is available, or the required rank condition is not met, we develop a strategy to test the null hypothesis of no causal effect.},
archivePrefix = {arXiv},
arxivId = {arXiv:1609.08816v4},
author = {Miao, Wang and Geng, Zhi and {Tchetgen Tchetgen}, Eric J.},
doi = {10.1093/biomet/asy038},
eprint = {arXiv:1609.08816v4},
file = {:Users/nrindtor/Documents/papers{\_}pile/oup-accepted-manuscript-2018.pdf:pdf},
issn = {14643510},
journal = {Biometrika},
keywords = {Confounder,Identification,Measurement error,Negative control,Proxy.},
number = {4},
pages = {987--993},
title = {{Identifying causal effects with proxy variables of an unmeasured confounder}},
volume = {105},
year = {2018}
}
@article{Garnelo2018,
archivePrefix = {arXiv},
arxivId = {arXiv:1807.01613v1},
author = {Garnelo, Marta and Rosenbaum, Dan and Maddison, Chris J and Ramalho, Tiago and Saxton, David and Shanahan, Murray and Whye, Yee and Danilo, Teh and Ali, J Rezende S M},
eprint = {arXiv:1807.01613v1},
file = {:Users/nrindtor/Documents/papers{\_}pile/1807.01613.pdf:pdf},
title = {{Conditional Neural Processes}},
year = {2018}
}
@article{Fox,
author = {Fox, Charles and Roberts, Stephen},
file = {:Users/nrindtor/Documents/papers{\_}pile/fox{\_}vbtut.pdf:pdf},
keywords = {mean-field,tutorial,variational bayes},
title = {{A Tutorial on Variational Bayesian Inference}}
}
@article{Iorio2016,
abstract = {Systematic studies of cancer genomes have provided unprecedented insights into the molecular nature of cancer. Using this information to guide the development and application of therapies in the clinic is challenging. Here, we report how cancer-driven alterations identified in 11,289 tumors from 29 tissues (integrating somatic mutations, copy number alterations, DNA methylation, and gene expression) can be mapped onto 1,001 molecularly annotated human cancer cell lines and correlated with sensitivity to 265 drugs. We find that cell lines faithfully recapitulate oncogenic alterations identified in tumors, find that many of these associate with drug sensitivity/resistance, and highlight the importance of tissue lineage in mediating drug response. Logic-based modeling uncovers combinations of alterations that sensitize to drugs, while machine learning demonstrates the relative importance of different data types in predicting drug response. Our analysis and datasets are rich resources to link genotypes with cellular phenotypes and to identify therapeutic options for selected cancer sub-populations.},
archivePrefix = {arXiv},
arxivId = {NIHMS150003},
author = {Iorio, Francesco and Knijnenburg, Theo A. and Vis, Daniel J. and Bignell, Graham R. and Menden, Michael P. and Schubert, Michael and Aben, Nanne and Gon{\c{c}}alves, Emanuel and Barthorpe, Syd and Lightfoot, Howard and Cokelaer, Thomas and Greninger, Patricia and van Dyk, Ewald and Chang, Han and de Silva, Heshani and Heyn, Holger and Deng, Xianming and Egan, Regina K. and Liu, Qingsong and Mironenko, Tatiana and Mitropoulos, Xeni and Richardson, Laura and Wang, Jinhua and Zhang, Tinghu and Moran, Sebastian and Sayols, Sergi and Soleimani, Maryam and Tamborero, David and Lopez-Bigas, Nuria and Ross-Macdonald, Petra and Esteller, Manel and Gray, Nathanael S. and Haber, Daniel A. and Stratton, Michael R. and Benes, Cyril H. and Wessels, Lodewyk F.A. and Saez-Rodriguez, Julio and McDermott, Ultan and Garnett, Mathew J.},
doi = {10.1016/j.cell.2016.06.017},
eprint = {NIHMS150003},
file = {:Users/nrindtor/Library/Application Support/Mendeley Desktop/Downloaded/Iorio et al. - 2016 - A Landscape of Pharmacogenomic Interactions in Cancer.pdf:pdf},
isbn = {1097-4172 (Electronic)$\backslash$r0092-8674 (Linking)},
issn = {10974172},
journal = {Cell},
number = {3},
pages = {740--754},
pmid = {27397505},
title = {{A Landscape of Pharmacogenomic Interactions in Cancer}},
volume = {166},
year = {2016}
}
@article{ShengyangSunGuodongZhangJiaxinShi2011,
archivePrefix = {arXiv},
arxivId = {arXiv:1810.03958v1},
author = {{Shengyang Sun∗∗†, Guodong Zhang∗∗†, Jiaxin Shi∗∗‡}, Roger Grosse†},
eprint = {arXiv:1810.03958v1},
file = {:Users/nrindtor/Documents/papers{\_}pile/21372b0863f222e660510f69cf19b7b3873e887d.pdf:pdf},
number = {April},
pages = {1--23},
title = {{FUNCTIONAL VARIATIONAL BAYESIAN NEURAL NETWORKS}},
year = {2011}
}
@article{Carpenter2017,
abstract = {Stan is a probabilistic programming language for specifying statistical models. A Stan program imperatively defines a log probability function over parameters conditioned on specified data and constants. As of version 2.14.0, Stan provides full Bayesian inference for continuous-variable models through Markov chain Monte Carlo methods such as the No-U-Turn sampler, an adaptive form of Hamiltonian Monte Carlo sampling. Penalized maximum likelihood estimates are calculated using optimization methods such as the limited memory Broyden-Fletcher-Goldfarb-Shanno algorithm. Stan is also a platform for computing log densities and their gradients and Hessians, which can be used in alternative algorithms such as variational Bayes, expectation propagation, and marginal inference using approximate integration. To this end, Stan is set up so that the densities, gradients, and Hessians, along with intermediate quantities of the algorithm such as acceptance probabilities, are easily accessible. Stan can be called from the command line using the cmdstan package, through R using the rstan package, and through Python using the pystan package. All three interfaces support sampling and optimization-based inference with diagnostics and posterior analysis. rstan and pystan also provide access to log probabilities, gradients, Hessians, parameter transforms, and specialized plotting.},
author = {Carpenter, Bob and Gelman, Andrew and Hoffman, Matthew D. and Lee, Daniel and Goodrich, Ben and Betancourt, Michael and Brubaker, Marcus and Guo, Jiqiang and Li, Peter and Riddell, Allen},
doi = {10.18637/jss.v076.i01},
issn = {1548-7660},
journal = {Journal of Statistical Software},
keywords = {Bayesian inference,Stan,algorithmic differentiation,probabilistic programming},
month = {jan},
number = {1},
pages = {1--32},
title = {{{\textless}i{\textgreater}Stan{\textless}/i{\textgreater} : A Probabilistic Programming Language}},
url = {http://www.jstatsoft.org/v76/i01/},
volume = {76},
year = {2017}
}
@article{Wang2019,
abstract = {Gaussian processes (GPs) are flexible models with state-of-the-art performance on many impactful applications. However, computational constraints with standard inference procedures have limited exact GPs to problems with fewer than about ten thousand training points, necessitating approximations for larger datasets. In this paper, we develop a scalable approach for exact GPs that leverages multi-GPU parallelization and methods like linear conjugate gradients, accessing the kernel matrix only through matrix multiplication. By partitioning and distributing kernel matrix multiplies, we demonstrate that an exact GP can be trained on over a million points in 3 days using 8 GPUs and can compute predictive means and variances in under a second using 1 GPU at test time. Moreover, we perform the first-ever comparison of exact GPs against state-of-the-art scalable approximations on large-scale regression datasets with {\$}10{\^{}}4-10{\^{}}6{\$} data points, showing dramatic performance improvements.},
archivePrefix = {arXiv},
arxivId = {1903.08114},
author = {Wang, Ke Alexander and Pleiss, Geoff and Gardner, Jacob R. and Tyree, Stephen and Weinberger, Kilian Q. and Wilson, Andrew Gordon},
eprint = {1903.08114},
file = {:Users/nrindtor/Documents/papers{\_}pile/1903.08114.pdf:pdf},
title = {{Exact Gaussian Processes on a Million Data Points}},
url = {http://arxiv.org/abs/1903.08114},
year = {2019}
}
@article{Lee2017,
abstract = {It has long been known that a single-layer fully-connected neural network with an i.i.d. prior over its parameters is equivalent to a Gaussian process (GP), in the limit of infinite network width. This correspondence enables exact Bayesian inference for infinite width neural networks on regression tasks by means of evaluating the corresponding GP. Recently, kernel functions which mimic multi-layer random neural networks have been developed, but only outside of a Bayesian framework. As such, previous work has not identified that these kernels can be used as covariance functions for GPs and allow fully Bayesian prediction with a deep neural network. In this work, we derive the exact equivalence between infinitely wide deep networks and GPs. We further develop a computationally efficient pipeline to compute the covariance function for these GPs. We then use the resulting GPs to perform Bayesian inference for wide deep neural networks on MNIST and CIFAR-10. We observe that trained neural network accuracy approaches that of the corresponding GP with increasing layer width, and that the GP uncertainty is strongly correlated with trained network prediction error. We further find that test performance increases as finite-width trained networks are made wider and more similar to a GP, and thus that GP predictions typically outperform those of finite-width networks. Finally we connect the performance of these GPs to the recent theory of signal propagation in random neural networks.},
archivePrefix = {arXiv},
arxivId = {1711.00165},
author = {Lee, Jaehoon and Bahri, Yasaman and Novak, Roman and Schoenholz, Samuel S. and Pennington, Jeffrey and Sohl-Dickstein, Jascha},
doi = {arXiv:1711.00165v3},
eprint = {1711.00165},
file = {:Users/nrindtor/Documents/papers{\_}pile/1711.00165.pdf:pdf},
isbn = {0022-1767 (Print)$\backslash$r0022-1767 (Linking)},
issn = {18785352},
pmid = {7594604},
title = {{Deep Neural Networks as Gaussian Processes}},
url = {http://arxiv.org/abs/1711.00165},
year = {2017}
}
@article{Osband2015,
abstract = {This technical note presents a new approach to carrying out the kind of exploration achieved by Thompson sampling, but without explicitly maintaining or sampling from posterior distributions. The approach is based on a bootstrap technique that uses a combination of observed and artificially generated data. The latter serves to induce a prior distribution which, as we will demonstrate, is critical to effective exploration. We explain how the approach can be applied to multi-armed bandit and reinforcement learning problems and how it relates to Thompson sampling. The approach is particularly well-suited for contexts in which exploration is coupled with deep learning, since in these settings, maintaining or generating samples from a posterior distribution becomes computationally infeasible.},
archivePrefix = {arXiv},
arxivId = {1507.00300},
author = {Osband, Ian and {Van Roy}, Benjamin},
eprint = {1507.00300},
file = {:Users/nrindtor/Documents/papers{\_}pile/1507.00300.pdf:pdf},
pages = {1--10},
title = {{Bootstrapped Thompson Sampling and Deep Exploration}},
url = {http://arxiv.org/abs/1507.00300},
year = {2015}
}
@misc{Letai2017,
abstract = {Anthony Letai proposes wider adoption of functional assays in efforts to match the right drug to the right patient and discusses why these assays might be complementary to existing genomics-based approaches.},
author = {Letai, Anthony},
booktitle = {Nature Medicine},
doi = {10.1038/nm.4389},
file = {:Users/nrindtor/Library/Application Support/Mendeley Desktop/Downloaded/Letai - 2017 - Functional precision cancer medicine — moving beyond pure genomics.pdf:pdf},
isbn = {1546-170X (Electronic) 1078-8956 (Linking)},
issn = {1546170X},
number = {9},
pages = {1028--1035},
pmid = {28886003},
publisher = {Nature Publishing Group},
title = {{Functional precision cancer medicine-moving beyond pure genomics}},
url = {http://dx.doi.org/10.1038/nm.4389},
volume = {23},
year = {2017}
}
@article{Powers2017,
archivePrefix = {arXiv},
arxivId = {arXiv:1707.00102v1},
author = {Powers, Scott and Qian, Junyang and Jung, Kenneth and Schuler, Alejandro and Shah, Nigam H and Hastie, Trevor and Tibshirani, Robert},
eprint = {arXiv:1707.00102v1},
file = {:Users/nrindtor/Documents/papers{\_}pile/1707.00102.pdf:pdf},
journal = {arxiv},
pages = {1--28},
title = {{Some methods for heterogeneous treatment effect estimation in high-dimensions}},
year = {2017}
}
@article{Hill2011,
abstract = {Researchers have long struggled to identify causal effects in nonexperimental settings. Many recently proposed strategies assume ignorability of the treatment assignment mechanism and require fitting two models—one for the assignment mechanism and one for the response surface. This article proposes a strategy that instead focuses on very flexibly modeling just the response surface using a Bayesian nonparametric modeling procedure, Bayesian Additive Regression Trees (BART). BART has several advantages: it is far simpler to use than many recent competitors, requires less guesswork in model fitting, handles a large number of predictors, yields coherent uncertainty intervals, and fluidly handles continuous treatment variables and missing data for the outcome variable. BART also naturally identifies heterogeneous treatment effects. BART produces more accurate estimates of average treatment effects compared to propensity score matching, propensity-weighted estimators, and regression adjustment in the nonlinear ...},
author = {Hill, Jennifer L.},
doi = {10.1198/jcgs.2010.08162},
issn = {1061-8600},
journal = {Journal of Computational and Graphical Statistics},
keywords = {Bayesian,Causal inference,Nonparametrics},
month = {jan},
number = {1},
pages = {217--240},
publisher = {Taylor {\&} Francis},
title = {{Bayesian Nonparametric Modeling for Causal Inference}},
url = {http://www.tandfonline.com/doi/abs/10.1198/jcgs.2010.08162},
volume = {20},
year = {2011}
}
@article{Hou2011,
abstract = {BACKGROUND: Phase 1 and 2 clinical trials of the BRAF kinase inhibitor vemurafenib (PLX4032) have shown response rates of more than 50{\%} in patients with metastatic melanoma with the BRAF V600E mutation.$\backslash$n$\backslash$nMETHODS: We conducted a phase 3 randomized clinical trial comparing vemurafenib with dacarbazine in 675 patients with previously untreated, metastatic melanoma with the BRAF V600E mutation. Patients were randomly assigned to receive either vemurafenib (960 mg orally twice daily) or dacarbazine (1000 mg per square meter of body-surface area intravenously every 3 weeks). Coprimary end points were rates of overall and progression-free survival. Secondary end points included the response rate, response duration, and safety. A final analysis was planned after 196 deaths and an interim analysis after 98 deaths.$\backslash$n$\backslash$nRESULTS: At 6 months, overall survival was 84{\%} (95{\%} confidence interval [CI], 78 to 89) in the vemurafenib group and 64{\%} (95{\%} CI, 56 to 73) in the dacarbazine group. In the interim analysis for overall survival and final analysis for progression-free survival, vemurafenib was associated with a relative reduction of 63{\%} in the risk of death and of 74{\%} in the risk of either death or disease progression, as compared with dacarbazine (P{\textless}0.001 for both comparisons). After review of the interim analysis by an independent data and safety monitoring board, crossover from dacarbazine to vemurafenib was recommended. Response rates were 48{\%} for vemurafenib and 5{\%} for dacarbazine. Common adverse events associated with vemurafenib were arthralgia, rash, fatigue, alopecia, keratoacanthoma or squamous-cell carcinoma, photosensitivity, nausea, and diarrhea; 38{\%} of patients required dose modification because of toxic effects.$\backslash$n$\backslash$nCONCLUSIONS: Vemurafenib produced improved rates of overall and progression-free survival in patients with previously untreated melanoma with the BRAF V600E mutation. (Funded by Hoffmann-La Roche; BRIM-3 ClinicalTrials.gov number, NCT01006980.).},
author = {Hou, Jeannie and Dreno, Brigitte and Maio, Michele and Haanen, John B and Chapman, Paul B. and Li, Jiang and Larkin, James and Nolop, Keith and Lee, Richard J and Ribas, Antoni and Flaherty, Keith T and Garbe, Claus and O'Day, Steven J. and Schadendorf, Dirk and Testori, Alessandro and Eggermont, Alexander M.M. and Nelson, Betty and Hauschild, Axel and Lorigan, Paul and Robert, Caroline and Hogg, David and Sosman, Jeffrey A and Lebbe, Celeste and Ascierto, Paolo and Kirkwood, John M. and McArthur, Grant A. and Dummer, Reinhard and Jouary, Thomas},
doi = {10.1056/nejmoa1103782},
file = {:Users/nrindtor/Documents/papers{\_}pile/nejmoa1103782.pdf:pdf},
isbn = {1533-4406 (Electronic)$\backslash$r0028-4793 (Linking)},
issn = {0028-4793},
journal = {New England Journal of Medicine},
number = {26},
pages = {2507--2516},
pmid = {21639808},
title = {{Improved Survival with Vemurafenib in Melanoma with BRAF V600E Mutation}},
volume = {364},
year = {2011}
}
@article{Yauney2018,
abstract = {Unstructured learning problems without well-defined rewards are unsuitable for current reinforcement learning (RL) approaches. Action-derived rewards can allow RL agents to fully explore state and action trade-offs in scenarios that require specific outcomes yet are unstructured by external reward. Clinical trial dosing choice is an example of such a problem. We report the successful formulation of clinical trial dosing choice as an RL problem using action-based rewards and learning of dosing regimens to reduce mean tumor diameters (MTD) in patients undergoing simulated temozolomide (TMZ) and procarbazine, 1-(2-chloroethyl)-3-cyclohexyl-l-nitrosourea, and vincristine (PCV) chemo-and radiother-apy clinical trials. The use of action-derived rewards as partial proxies for outcomes is described for the first time. Novel dosing regimens learned by an RL agent in the presence of action-derived rewards achieve significant reduction in MTD for cohorts and individual patients in simulated TMZ and PCV clinical trials while reducing treatment cycle administrations and dosage concentrations compared to human-expert dosing regimens. Our approach can be easily adapted for other learning tasks where outcome-based learning is not practical.},
author = {Yauney, Gregory and Shah, Pratik},
file = {:Users/nrindtor/Documents/papers{\_}pile/reinforcement{\_}learning{\_}with{\_}action{\_}derived{\_}rewards{\_}for{\_}chemotherapy{\_}and{\_}clinical{\_}trial{\_}dosing{\_}regimen{\_}selection.pdf:pdf},
journal = {Mlhc-2018},
pages = {2018},
title = {{Reinforcement Learning with Action-Derived Rewards for Chemotherapy and Clinical Trial Dosing Regimen Selection}},
url = {http://web.media.mit.edu/{~}pratiks/mlhc{\_}2018/reinforcement{\_}learning{\_}with{\_}action{\_}derived{\_}rewards{\_}for{\_}chemotherapy{\_}and{\_}clinical{\_}trial{\_}dosing{\_}regimen{\_}selection.pdf},
volume = {85},
year = {2018}
}
@article{Milch2007,
abstract = {... 386 BLOG : Probabilistic  Models with Unknown  Objects Note that some RVs, such as{\#} Ball [] in ... $\backslash$nThe well-deﬁnedness criterion for Blog is similar, but deals with ﬁnite, self ... 388 BLOG : Probabilistic Models with Unknown  Objects 13.5 Evidence and Queries Because a well-deﬁned ... $\backslash$n},
author = {Milch, Brian and Marthi, Bhaskara and Russel, Stuart and Sontag, David and Ong, Daniel L. and Kolobov, Andrey},
file = {:Users/nrindtor/Documents/papers{\_}pile/ijcai05-blog.pdf:pdf},
journal = {Statistical Relational Learning},
keywords = {first-order logic,iden-,knowledge representation,probability},
pages = {352},
title = {{Probabilistic models with unknown objects}},
year = {2007}
}
@article{Russo2017,
abstract = {Thompson sampling is an algorithm for online decision problems where actions are taken sequentially in a manner that must balance between exploiting what is known to maximize immediate performance and investing to accumulate new information that may improve future performance. The algorithm addresses a broad range of problems in a computationally efficient manner and is therefore enjoying wide use. This tutorial covers the algorithm and its application, illustrating concepts through a range of examples, including Bernoulli bandit problems, shortest path problems, dynamic pricing, recommendation, active learning with neural networks, and reinforcement learning in Markov decision processes. Most of these problems involve complex information structures, where information revealed by taking an action informs beliefs about other actions. We will also discuss when and why Thompson sampling is or is not effective and relations to alternative algorithms.},
archivePrefix = {arXiv},
arxivId = {1707.02038},
author = {Russo, Daniel and {Van Roy}, Benjamin and Kazerouni, Abbas and Osband, Ian and Wen, Zheng},
doi = {10.1561/2200000070},
eprint = {1707.02038},
file = {:Users/nrindtor/Documents/papers{\_}pile/TS{\_}Tutorial.pdf:pdf},
isbn = {9781680833683},
issn = {1935-8237},
pages = {1--96},
title = {{A Tutorial on Thompson Sampling}},
url = {http://arxiv.org/abs/1707.02038},
year = {2017}
}
@article{Ledermann2014,
abstract = {Background: Maintenance monotherapy with the PARP inhibitor olaparib significantly prolonged progression-free survival (PFS) versus placebo in patients with platinum-sensitive recurrent serous ovarian cancer. We aimed to explore the hypothesis that olaparib is most likely to benefit patients with a BRCA mutation. Methods: We present data from the second interim analysis of overall survival and a retrospective, preplanned analysis of data by BRCA mutation status from our randomised, double-blind, phase 2 study that assessed maintenance treatment with olaparib 400 mg twice daily (capsules) versus placebo in patients with platinum-sensitive recurrent serous ovarian cancer who had received two or more platinum-based regimens and who had a partial or complete response to their most recent platinum-based regimen. Randomisation was by an interactive voice response system, stratified by time to progression on penultimate platinum-based regimen, response to the most recent platinum-based regimen before randomisation, and ethnic descent. The primary endpoint was PFS, analysed for the overall population and by BRCA status. This study is registered with ClinicalTrials.gov, number NCT00753545. Findings: Between Aug 28, 2008, and Feb 9, 2010, 136 patients were assigned to olaparib and 129 to placebo. BRCA status was known for 131 (96{\%}) patients in the olaparib group versus 123 (95{\%}) in the placebo group, of whom 74 (56{\%}) versus 62 (50{\%}) had a deleterious or suspected deleterious germline or tumour BRCA mutation. Of patients with a BRCA mutation, median PFS was significantly longer in the olaparib group than in the placebo group (11{\textperiodcentered}2 months [95{\%} CI 8{\textperiodcentered}3-not calculable] vs 4{\textperiodcentered}3 months [3{\textperiodcentered}0-5{\textperiodcentered}4]; HR 0{\textperiodcentered}18 [0{\textperiodcentered}10-0{\textperiodcentered}31]; p{\textless}0{\textperiodcentered}0001); similar findings were noted for patients with wild-type BRCA, although the difference between groups was lower (7{\textperiodcentered}4 months [5{\textperiodcentered}5-10{\textperiodcentered}3] vs 5{\textperiodcentered}5 months [3{\textperiodcentered}7-5{\textperiodcentered}6]; HR 0{\textperiodcentered}54 [0{\textperiodcentered}34-0{\textperiodcentered}85]; p=0{\textperiodcentered}0075). At the second interim analysis of overall survival (58{\%} maturity), overall survival did not significantly differ between the groups (HR 0{\textperiodcentered}88 [95{\%} CI 0{\textperiodcentered}64-1{\textperiodcentered}21]; p=0{\textperiodcentered}44); similar findings were noted for patients with mutated BRCA (HR 0{\textperiodcentered}73 [0{\textperiodcentered}45-1{\textperiodcentered}17]; p=0{\textperiodcentered}19) and wild-type BRCA (HR 0{\textperiodcentered}99 [0{\textperiodcentered}63-1{\textperiodcentered}55]; p=0{\textperiodcentered}96). The most common grade 3 or worse adverse events in the olaparib group were fatigue (in ten [7{\%}] patients in the olaparib group vs four [3{\%}] in the placebo group) and anaemia (seven [5{\%}] vs one [{\textless}1{\%}]). Serious adverse events were reported in 25 (18{\%}) patients who received olaparib and 11 (9{\%}) who received placebo. Tolerability was similar in patients with mutated BRCA and the overall population. Interpretation: These results support the hypothesis that patients with platinum-sensitive recurrent serous ovarian cancer with a BRCA mutation have the greatest likelihood of benefiting from olaparib treatment. {\textcopyright} 2014 Elsevier Ltd.},
author = {Ledermann, Jonathan and Harter, Philipp and Gourley, Charlie and Friedlander, Michael and Vergote, Ignace and Rustin, Gordon and Scott, Clare L and Meier, Werner and Shapira-Frommer, Ronnie and Safra, Tamar and Matei, Daniela and Fielding, Anitra and Spencer, Stuart and Dougherty, Brian and Orr, Maria and Hodgson, Darren and Barrett, J Carl and Matulonis, Ursula},
doi = {10.1016/S1470-2045(14)70228-1},
file = {:Users/nrindtor/Documents/papers{\_}pile/ledermann2014.pdf:pdf},
isbn = {1474-5488 (Electronic)$\backslash$r1470-2045 (Linking)},
issn = {14745488},
journal = {The Lancet Oncology},
number = {8},
pages = {852--861},
pmid = {24882434},
publisher = {Elsevier Ltd},
title = {{Olaparib maintenance therapy in patients with platinum-sensitive relapsed serous ovarian cancer: A preplanned retrospective analysis of outcomes by BRCA status in a randomised phase 2 trial}},
url = {http://dx.doi.org/10.1016/S1470-2045(14)70228-1},
volume = {15},
year = {2014}
}
@article{Yada2017,
author = {Yada, Shinjo and Hamada, Chikuma},
doi = {10.1002/pst.1793},
file = {:Users/nrindtor/Documents/papers{\_}pile/yada2016.pdf:pdf},
issn = {15391612},
journal = {Pharmaceutical Statistics},
keywords = {bayesian adaptive randomization,combination therapy,seamless phase I/II trials},
number = {2},
pages = {114--121},
title = {{Application of Bayesian hierarchical models for phase I/II clinical trials in oncology}},
volume = {16},
year = {2017}
}
@article{Kim2018,
abstract = {Learning to infer Bayesian posterior from a few-shot dataset is an important step towards robust meta-learning due to the model uncertainty inherent in the problem. In this paper, we propose a novel Bayesian model-agnostic meta-learning method. The proposed method combines scalable gradient-based meta-learning with nonparametric variational inference in a principled probabilistic framework. During fast adaptation, the method is capable of learning complex uncertainty structure beyond a point estimate or a simple Gaussian approximation. In addition, a robust Bayesian meta-update mechanism with a new meta-loss prevents overfitting during meta-update. Remaining an efficient gradient-based meta-learner, the method is also model-agnostic and simple to implement. Experiment results show the accuracy and robustness of the proposed method in various tasks: sinusoidal regression, image classification, active learning, and reinforcement learning.},
archivePrefix = {arXiv},
arxivId = {1806.03836},
author = {Kim, Taesup and Yoon, Jaesik and Dia, Ousmane and Kim, Sungwoong and Bengio, Yoshua and Ahn, Sungjin},
doi = {arXiv:1806.03836v2},
eprint = {1806.03836},
file = {:Users/nrindtor/Documents/papers{\_}pile/1806.03836.pdf:pdf},
pages = {1--16},
pmid = {1232206},
title = {{Bayesian Model-Agnostic Meta-Learning}},
url = {http://arxiv.org/abs/1806.03836},
year = {2018}
}
@article{Imai2013,
abstract = {When evaluating the efficacy of social programs and medical treatments using randomized experiments, the estimated overall average causal effect alone is often of limited value and the researchers must investigate when the treatments do and do not work. Indeed, the estimation of treatment effect heterogeneity plays an essential role in (1) selecting the most effective treatment from a large number of available treatments, (2) ascertaining subpopulations for which a treatment is effective or harmful, (3) designing individualized optimal treatment regimes, (4) testing for the existence or lack of heterogeneous treatment effects, and (5) generalizing causal effect estimates obtained from an experimental sample to a target population. In this paper, we formulate the estimation of heterogeneous treatment effects as a variable selection problem. We propose a method that adapts the Support Vector Machine classifier by placing separate sparsity constraints over the pre-treatment parameters and causal heterogeneity parameters of interest. The proposed method is motivated by and applied to two well-known randomized evaluation studies in the social sciences. Our method selects the most effective voter mobilization strategies from a large number of alternative strategies, and it also identifies the characteristics of workers who greatly benefit from (or are negatively affected by) a job training program. In our simulation studies, we find that the proposed method often outperforms some commonly used alternatives.},
archivePrefix = {arXiv},
arxivId = {arXiv:1305.5682v1},
author = {Imai, Kosuke and Ratkovic, Marc},
doi = {10.1214/12-AOAS593},
eprint = {arXiv:1305.5682v1},
file = {:Users/nrindtor/Documents/papers{\_}pile/imai2013.pdf:pdf},
isbn = {1932-6157},
issn = {19326157},
journal = {Annals of Applied Statistics},
keywords = {Causal inference,Individualized treatment rules,LASSO,Moderation,Variable selection},
number = {1},
pages = {443--470},
title = {{Estimating treatment effect heterogeneity in randomized program evaluation}},
volume = {7},
year = {2013}
}
@article{Finn2017,
abstract = {We propose an algorithm for meta-learning that is model-agnostic, in the sense that it is compatible with any model trained with gradient descent and applicable to a variety of different learning problems, including classification, regression, and reinforcement learning. The goal of meta-learning is to train a model on a variety of learning tasks, such that it can solve new learning tasks using only a small number of training samples. In our approach, the parameters of the model are explicitly trained such that a small number of gradient steps with a small amount of training data from a new task will produce good generalization performance on that task. In effect, our method trains the model to be easy to fine-tune. We demonstrate that this approach leads to state-of-the-art performance on two few-shot image classification benchmarks, produces good results on few-shot regression, and accelerates fine-tuning for policy gradient reinforcement learning with neural network policies.},
archivePrefix = {arXiv},
arxivId = {1703.03400},
author = {Finn, Chelsea and Abbeel, Pieter and Levine, Sergey},
doi = {10.5025/hansen.76.245},
eprint = {1703.03400},
file = {:Users/nrindtor/Documents/papers{\_}pile/1703.03400.pdf:pdf},
isbn = {9781510855144},
issn = {0021-9258},
pmid = {1939184},
title = {{Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks}},
url = {http://arxiv.org/abs/1703.03400},
year = {2017}
}
@article{Blei2018,
archivePrefix = {arXiv},
arxivId = {arXiv:1601.00670v9},
author = {Blei, David M and Kucukelbir, Alp and Mcauliffe, Jon D},
eprint = {arXiv:1601.00670v9},
file = {:Users/nrindtor/Documents/papers{\_}pile/1601.00670.pdf:pdf},
keywords = {algorithms,computationally intensive methods,statistical computing},
pages = {1--41},
title = {{Variational Inference : A Review for Statisticians}},
year = {2018}
}
@article{Ziegler2018,
author = {Ziegler, Zachary},
file = {:Users/nrindtor/Documents/papers{\_}pile/FlowOverviewSummer.pdf:pdf},
number = {July},
pages = {1--9},
title = {{Normalizing Flow Overview Autoregressive normalizing flows}},
volume = {2},
year = {2018}
}
@article{Snoek2015,
abstract = {Bayesian optimization is an effective methodology for the global optimization of functions with expensive evaluations. It relies on querying a distribution over functions defined by a relatively cheap surrogate model. An accurate model for this distribution over functions is critical to the effectiveness of the approach, and is typically fit using Gaussian processes (GPs). However, since GPs scale cubically with the number of observations, it has been challenging to handle objectives whose optimization requires many evaluations, and as such, massively parallelizing the optimization. In this work, we explore the use of neural networks as an alternative to GPs to model distributions over functions. We show that performing adaptive basis function regression with a neural network as the parametric form performs competitively with state-of-the-art GP-based approaches, but scales linearly with the number of data rather than cubically. This allows us to achieve a previously intractable degree of parallelism, which we apply to large scale hyperparameter optimization, rapidly finding competitive models on benchmark object recognition tasks using convolutional networks, and image caption generation using neural language models.},
archivePrefix = {arXiv},
arxivId = {1502.05700},
author = {Snoek, Jasper and Rippel, Oren and Swersky, Kevin and Kiros, Ryan and Satish, Nadathur and Sundaram, Narayanan and Patwary, Md. Mostofa Ali and Prabhat and Adams, Ryan P.},
doi = {10.1002/j.2161-1912.1997.tb00313.x},
eprint = {1502.05700},
file = {:Users/nrindtor/Documents/papers{\_}pile/1502.05700.pdf:pdf},
isbn = {9781510810587},
issn = {1938-7228},
title = {{Scalable Bayesian Optimization Using Deep Neural Networks}},
url = {http://arxiv.org/abs/1502.05700},
year = {2015}
}
@article{Kingma,
archivePrefix = {arXiv},
arxivId = {arXiv:1807.03039v2},
author = {Kingma, Diederik P and Dhariwal, Prafulla and Francisco, San},
eprint = {arXiv:1807.03039v2},
file = {:Users/nrindtor/Documents/papers{\_}pile/1807.03039.pdf:pdf},
pages = {1--15},
title = {{Glow: Generative Flow with Invertible 1×1 Convolutions Diederik}}
}
@article{Grathwohl2019,
archivePrefix = {arXiv},
arxivId = {arXiv:1810.01367v3},
author = {Grathwohl, Will and Chen, Ricky T Q and Bettencourt, Jesse and Sutskever, Ilya and Duvenaud, David},
eprint = {arXiv:1810.01367v3},
file = {:Users/nrindtor/Documents/papers{\_}pile/1810.01367.pdf:pdf},
pages = {1--13},
title = {{FFJORD: FREE-FORM CONTINUOUS DYNAMICS FOR SCALABLE REVERSIBLE GENERATIVE MODELS}},
year = {2019}
}
@article{Sridhara2015,
abstract = {C2 oxygenate (acetaldehyde, ethanol, etc.) formation from syngas (CO + H2) is an important industrial process for the production of clean liquid energy fuels and valuable chemical feedstocks that are catalyzed industrially by Rh modified with Mn and Fe, etc. In an effort to identify catalysts based on less expensive metals and higher C2 oxygenate selectivity, density functional theory (DFT) calculations were performed to tune the relative activity of the selectivity-determining steps, i.e., CO insertion in CHx (x = 1, 2, 3) versus CHx hydrogenation by changing composition and structure of material. We find that the Rh-decorated Cu alloy catalyst has significantly lower CO insertion barriers compared to pristine Rh(111) and vicinal Rh(553) surfaces, whereas the variation of CHx hydrogenation barriers on the three surfaces is modest. A semiquantitative kinetic analysis based on DFT calculations shows that the C2 oxygenate selectivity on RhCu(111) is substantially improved, with the production rate of C2 oxygenates slightly higher than CH4 under experimental conditions, compared with Rh(111) and Rh(553) that are highly selective to CH4. Our calculations suggest that the improved C 2 oxygenate selectivity on the RhCu alloy is primarily due to the fact that CO insertion is rather sensitive, whereas hydrogenation is insensitive to the ensemble effect. Furthermore, the Rh-decorated Cu alloy has stronger resistance toward coking and lower constituent cost compared to pure Rh catalysts and is thus a promising candidate for an improved C2 oxygenate synthesis catalyst.},
author = {Sridhara, Rajeshwari and He, Kun and Nie, Lei and Shen, Yuan Li and Tang, Shenghui},
doi = {10.1080/19466315.2015.1094673},
file = {:Users/nrindtor/Documents/papers{\_}pile/sridhara2015.pdf:pdf},
issn = {19466315},
journal = {Statistics in Biopharmaceutical Research},
keywords = {Expansion cohorts,Master protocols,Non-proportional hazards,Treatment switch-over},
number = {4},
pages = {348--356},
title = {{Current Statistical Challenges in Oncology Clinical Trials in the Era of Targeted Therapy}},
volume = {7},
year = {2015}
}
@article{Tucci2013,
abstract = {This is a purely pedagogical paper with no new results. The goal of the paper is to give a fairly self-contained introduction to Judea Pearl's do-calculus, including proofs of his 3 rules.},
archivePrefix = {arXiv},
arxivId = {1305.5506},
author = {Tucci, Robert R.},
eprint = {1305.5506},
file = {:Users/nrindtor/Documents/papers{\_}pile/1305.5506.pdf:pdf},
title = {{Introduction to Judea Pearl's Do-Calculus}},
url = {http://arxiv.org/abs/1305.5506},
year = {2013}
}
@article{Johansson2018,
abstract = {Predictive models that generalize well under distributional shift are often desirable and sometimes crucial to building robust and reliable machine learning applications. We focus on distributional shift that arises in causal inference from observational data and in unsupervised domain adaptation. We pose both of these problems as prediction under a shift in design. Popular methods for overcoming distributional shift make unrealistic assumptions such as having a well-specified model or knowing the policy that gave rise to the observed data. Other methods are hindered by their need for a pre-specified metric for comparing observations, or by poor asymptotic properties. We devise a bound on the generalization error under design shift, incorporating both representation learning and sample re-weighting. Based on the bound, we propose an algorithmic framework that does not require any of the above assumptions and which is asymptotically consistent. We empirically study the new framework using two synthetic datasets, and demonstrate its effectiveness compared to previous methods.},
archivePrefix = {arXiv},
arxivId = {1802.08598},
author = {Johansson, Fredrik D. and Kallus, Nathan and Shalit, Uri and Sontag, David},
eprint = {1802.08598},
file = {:Users/nrindtor/Documents/papers{\_}pile/1802.08598.pdf:pdf},
title = {{Learning Weighted Representations for Generalization Across Designs}},
url = {http://arxiv.org/abs/1802.08598},
year = {2018}
}
@article{Chernozhukov2018,
abstract = {We revisit the classic semi{\&}{\#}8208;parametric problem of inference on a low{\&}{\#}8208;dimensional parameter {\&}{\#}952; in the presence of high{\&}{\#}8208;dimensional nuisance parameters {\&}{\#}951;. We depart from the classical setting by allowing for {\&}{\#}951; to be so high{\&}{\#}8208;dimensional that the traditional assumptions (e.g. Donsker properties) that limit complexity of the parameter space for this object break down. To estimate {\&}{\#}951;, we consider the use of statistical or machine learning (ML) methods, which are particularly well suited to estimation in modern, very high{\&}{\#}8208;dimensional cases. ML methods perform well by employing regularization to reduce variance and trading off regularization bias with overfitting in practice. However, both regularization bias and overfitting in estimating {\&}{\#}951; cause a heavy bias in estimators of {\&}{\#}952; that are obtained by naively plugging ML estimators of {\&}{\#}951; into estimating equations for {\&}{\#}952;. This bias results in the naive estimator failing to be  consistent, where N is the sample size. We show that the impact of regularization bias and overfitting on estimation of the parameter of interest {\&}{\#}952; can be removed by using two simple, yet critical, ingredients: (1) using Neyman{\&}{\#}8208;orthogonal moments/scores that have reduced sensitivity with respect to nuisance parameters to estimate {\&}{\#}952;; (2) making use of cross{\&}{\#}8208;fitting, which provides an efficient form of data{\&}{\#}8208;splitting. We call the resulting set of methods double or debiased ML (DML). We verify that DML delivers point estimators that concentrate in an {\&}{\#}8208;neighbourhood of the true parameter values and are approximately unbiased and normally distributed, which allows construction of valid confidence statements. The generic statistical theory of DML is elementary and simultaneously relies on only weak theoretical requirements, which will admit the use of a broad array of modern ML methods for estimating the nuisance parameters, such as random forests, lasso, ridge, deep neural nets, boosted trees, and various hybrids and ensembles of these methods. We illustrate the general theory by applying it to provide theoretical properties of the following: DML applied to learn the main regression parameter in a partially linear regression model; DML applied to learn the coefficient on an endogenous variable in a partially linear instrumental variables model; DML applied to learn the average treatment effect and the average treatment effect on the treated under unconfoundedness; DML applied to learn the local average treatment effect in an instrumental variables setting. In addition to these theoretical applications, we also illustrate the use of DML in three empirical examples.},
archivePrefix = {arXiv},
arxivId = {1608.00060},
author = {Chernozhukov, Victor and Chetverikov, Denis and Demirer, Mert and Duflo, Esther and Hansen, Christian and Newey, Whitney and Robins, James},
doi = {10.1111/ectj.12097},
eprint = {1608.00060},
file = {:Users/nrindtor/Documents/papers{\_}pile/Chernozhukov{\_}et{\_}al-2018-The{\_}Econometrics{\_}Journal.pdf:pdf},
issn = {1368423X},
journal = {Econometrics Journal},
number = {1},
pages = {C1--C68},
title = {{Double/debiased machine learning for treatment and structural parameters}},
volume = {21},
year = {2018}
}
@article{Using2017,
archivePrefix = {arXiv},
arxivId = {arXiv:1605.08803v3},
author = {Using, Ensity Estimation},
eprint = {arXiv:1605.08803v3},
file = {:Users/nrindtor/Documents/papers{\_}pile/1605.08803.pdf:pdf},
title = {{DENSITY ESTIMATION USING REAL NVP}},
year = {2017}
}
@article{Liu2018,
abstract = {We study the problem of off-policy policy evaluation (OPPE) in RL. In contrast to prior work, we consider how to estimate both the individual policy value and average policy value accurately. We draw inspiration from recent work in causal reasoning, and propose a new finite sample generalization error bound for value estimates from MDP models. Using this upper bound as an objective, we develop a learning algorithm of an MDP model with a balanced representation, and show that our approach can yield substantially lower MSE in common synthetic benchmarks and a HIV treatment simulation domain.},
annote = {Understand the mapping between the CATE estimation problem and the RL OPPE problem},
archivePrefix = {arXiv},
arxivId = {1805.09044},
author = {Liu, Yao and Gottesman, Omer and Raghu, Aniruddh and Komorowski, Matthieu and Faisal, Aldo and Doshi-Velez, Finale and Brunskill, Emma},
doi = {10.1016/j.amepre.2016.04.011},
eprint = {1805.09044},
file = {:Users/nrindtor/Documents/papers{\_}pile/1805.09044.pdf:pdf},
issn = {1873-2607},
number = {Nips},
pmid = {27288289},
title = {{Representation Balancing MDPs for Off-Policy Policy Evaluation}},
url = {http://arxiv.org/abs/1805.09044},
year = {2018}
}
@article{Hahn2017,
abstract = {This paper develops a semi-parametric Bayesian regression model for estimating heterogeneous treatment effects from observational data. Standard nonlinear regression models, which may work quite well for prediction, can yield badly biased estimates of treatment effects when fit to data with strong confounding. Our Bayesian causal forests model avoids this problem by directly incorporating an estimate of the propensity function in the specification of the response model, implicitly inducing a covariate-dependent prior on the regression function. This new parametrization also allows treatment heterogeneity to be regularized separately from the prognostic effect of control variables, making it possible to informatively “shrink to homogeneity”, in contrast to existing Bayesian non- and semi-parametric approaches.},
archivePrefix = {arXiv},
arxivId = {1706.09523},
author = {Hahn, P. Richard and Murray, Jared and Carvalho, Carlos M.},
doi = {10.2139/ssrn.3048177},
eprint = {1706.09523},
file = {:Users/nrindtor/Documents/papers{\_}pile/1706.09523.pdf:pdf},
journal = {Ssrn},
keywords = {Bayesian,C11,C14,C30,C45,causal inference,heterogeneous effects,nonlinear regression,nonparametric regression,subgroup effects,treatment effects,tree methods},
number = {2011},
pages = {1--31},
title = {{Bayesian Regression Tree Models for Causal Inference: Regularization, Confounding, and Heterogeneous Effects}},
year = {2017}
}
@article{Louizos2017,
archivePrefix = {arXiv},
arxivId = {arXiv:1703.01961v2},
author = {Louizos, Christos and Welling, Max},
eprint = {arXiv:1703.01961v2},
file = {:Users/nrindtor/Documents/papers{\_}pile/1703.01961.pdf:pdf},
title = {{Multiplicative Normalizing Flows for Variational Bayesian Neural Networks}},
year = {2017}
}
@article{Outcomes2017,
author = {Outcomes, Potential},
file = {:Users/nrindtor/Documents/papers{\_}pile/causality{\_}the{\_}basic{\_}framework.pdf:pdf},
isbn = {9781139025751},
number = {1897},
pages = {3--22},
title = {{1.0 Causality : The Basic Framework}},
year = {2017}
}
@article{Garnelo2018a,
archivePrefix = {arXiv},
arxivId = {arXiv:1807.01622v1},
author = {Garnelo, Marta and Schwarz, Jonathan and Rosenbaum, Dan and Viola, Fabio and Rezende, Danilo J and Eslami, S M Ali and Teh, Yee Whye},
eprint = {arXiv:1807.01622v1},
file = {:Users/nrindtor/Documents/papers{\_}pile/1807.01622.pdf:pdf},
title = {{Neural Processes}},
year = {2018}
}
@article{DAmour2019,
abstract = {Unobserved confounding is a central barrier to drawing causal inferences from observational data. Several authors have recently proposed that this barrier can be overcome in the case where one attempts to infer the effects of several variables simultaneously. In this paper, we present two simple, analytical counterexamples that challenge the general claims that are central to these approaches. In addition, we show that nonparametric identification is impossible in this setting. We discuss practical implications, and suggest alternatives to the methods that have been proposed so far in this line or work: using proxy variables and shifting focus to sensitivity analysis.},
archivePrefix = {arXiv},
arxivId = {1902.10286},
author = {D'Amour, Alexander},
eprint = {1902.10286},
file = {:Users/nrindtor/Documents/papers{\_}pile/1902.10286 (1).pdf:pdf},
title = {{On Multi-Cause Causal Inference with Unobserved Confounding: Counterexamples, Impossibility, and Alternatives}},
url = {http://arxiv.org/abs/1902.10286},
volume = {89},
year = {2019}
}
@article{Wheldon2006,
abstract = {Objective: Ambient temperature alters exercise induced GH secretion. It is unknown whether temperature affects GH secretion at exercise intensities above the anaerobic threshold when other factors may override the relationship seen at lower intensities. Methods: . Design: Cross-over study of ambient temperature on exercise induced GH in swimmers and rowers. Setting: St Thomas Hospital, London. Subjects: Ten healthy men (age 21.7 ± 0.8 yrs). Five swimmers and five rowers. Intervention: Forty-minute exercise test at 105{\%} of anaerobic threshold at room temperature (RT) and at 4 °C. Measurements: Cutaneous and core body temperature. Serum GH concentration. Results: Cutaneous body temperature increased during exercise at RT but decreased in the cold. Although core temperature rose in both settings, the rise was greater at RT (p = 0.021). GH increased at both temperatures but the onset was delayed by the cold. Peak GH tended to be higher at RT (17.4 ± 3.6 $\mu$g/L vs. 9.5 ± 1.5 $\mu$g/L, p = 0.07). Total GH secretion was greater at RT (353.3 ± 99.1 $\mu$g min/L) than 4 °C (128.3 ± 21.0 $\mu$g min/L), p = 0.038. Change in core temperature correlated with log peak GH (r = 0.66, p = 0.039) and log incremental GH (r = 0.67, p = 0.032) when exercising at 4 °C. There was no difference between swimmers and rowers. Conclusions: Exercise at 4 °C reduces GH secretion during exercise at intensities above the anaerobic threshold. A change in core body temperature may be one mechanism by which exercise induces GH secretion. The difference in GH between swimmers and rowers during their respective events relates to the conditions under which they compete. {\textcopyright} 2006 Elsevier Ltd. All rights reserved.},
archivePrefix = {arXiv},
arxivId = {1801.08930},
author = {Wheldon, Adam and Savine, Richard L. and S{\"{o}}nksen, Peter H. and Holt, Richard I G},
doi = {10.1016/j.ghir.2006.02.005},
eprint = {1801.08930},
file = {:Users/nrindtor/Documents/papers{\_}pile/1801.08930.pdf:pdf},
isbn = {9781424438617},
issn = {10966374},
journal = {Growth Hormone and IGF Research},
keywords = {Growth hormone,Rowing,Swimming,Temperature},
number = {2},
pages = {125--131},
pmid = {16644256},
title = {{Exercising in the cold inhibits growth hormone secretion by reducing the rise in core body temperature}},
url = {http://arxiv.org/abs/1801.08930},
volume = {16},
year = {2006}
}
@article{Santoro2011,
abstract = {Fragile X syndrome (FXS) is a common form of inherited intellectual disability and is one of the leading known causes of autism. The mutation responsible for FXS is a large expansion of the trinucleotide CGG repeat in the 5' untranslated region of the X-linked gene FMR1. This expansion leads to DNA methylation of FMR1 and to transcriptional silencing, which results in the absence of the gene product, FMRP, a selective messenger RNA (mRNA)-binding protein that regulates the translation of a subset of dendritic mRNAs. FMRP is critical for mGluR (metabotropic glutamate receptor)-dependent long-term depression, as well as for other forms of synaptic plasticity; its absence causes excessive and persistent protein synthesis in postsynaptic dendrites and dysregulated synaptic function. Studies continue to refine our understanding of FMRP's role in synaptic plasticity and to uncover new functions of this protein, which have illuminated therapeutic approaches for FXS.},
annote = {Shalit et al: Get an intuition for each of the terms in the counterfactual loss in Lemma 1, and in the objective in (3).},
archivePrefix = {arXiv},
arxivId = {1606.03976},
author = {Santoro, Michael R. and Bray, Steven M. and Warren, Stephen T.},
doi = {10.1146/annurev-pathol-011811-132457},
eprint = {1606.03976},
file = {:Users/nrindtor/Library/Application Support/Mendeley Desktop/Downloaded/Shalit, Johansson, Sontag - 2016 - Estimating individual treatment effect generalization bounds and algorithms.pdf:pdf},
isbn = {1553-4014 (Electronic)$\backslash$r1553-4006 (Linking)},
issn = {1553-4006},
journal = {Annual Review of Pathology: Mechanisms of Disease},
number = {1},
pages = {219--245},
pmid = {22017584},
title = {{Molecular Mechanisms of Fragile X Syndrome: A Twenty-Year Perspective}},
url = {http://arxiv.org/abs/1606.03976},
volume = {7},
year = {2011}
}
@article{Kim2019,
archivePrefix = {arXiv},
arxivId = {arXiv:1901.05761v1},
author = {Kim, Hyunjik and Mnih, Andriy and Schwarz, Jonathan and Garnelo, Marta and Eslami, Ali and Rosenbaum, Dan and Vinyals, Oriol and Teh, Yee Whye},
eprint = {arXiv:1901.05761v1},
file = {:Users/nrindtor/Documents/papers{\_}pile/1901.05761.pdf:pdf},
pages = {1--18},
title = {{Attentive Neural Processes}},
year = {2019}
}
@techreport{VanDeMeent2018,
archivePrefix = {arXiv},
arxivId = {1809.10756v1},
author = {{Van De Meent}, Jan-Willem and Paige, Brooks and Yang, Hongseok and Wood, Frank},
eprint = {1809.10756v1},
file = {:Users/nrindtor/Library/Application Support/Mendeley Desktop/Downloaded/Van De Meent et al. - 2018 - An Introduction to Probabilistic Programming.pdf:pdf},
title = {{An Introduction to Probabilistic Programming}},
url = {https://arxiv.org/pdf/1809.10756.pdf},
year = {2018}
}
@article{Legg2008,
abstract = {This thesis concerns the optimal behaviour of agents in unknown computable environments, also known as universal artificial intelligence. These theoretical agents are able to learn to perform optimally in many types of environments. Although they are able to optimally use ...},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Legg, Shane},
doi = {10.1017/CBO9781107415324.004},
eprint = {arXiv:1011.1669v3},
file = {:Users/nrindtor/Documents/papers{\_}pile/Machine{\_}Super{\_}Intelligence.pdf:pdf},
isbn = {9781634393973},
issn = {1098-6596},
journal = {Doctoral Dissertation},
keywords = {AIXI, AI formalism},
number = {June},
pmid = {25246403},
title = {{Machine Super Intelligence - Thesis}},
url = {http://www.vetta.org/documents/Machine{\_}Super{\_}Intelligence.pdf},
year = {2008}
}
@article{Liang2010,
abstract = {Abstract We are interested in learning programs for multiple related tasks given only a few training examples per task. Since the program for a single task is underdetermined by its data, we introduce a nonparametric hierarchical Bayesian prior over programs which ... $\backslash$n},
author = {Liang, Percy and Jordan, Michael I and Klein, Dan},
file = {:Users/nrindtor/Documents/papers{\_}pile/programs-icml2010.pdf:pdf},
journal = {Proceedings of the 27th International Conference on Machine Learning (ICML-10)},
pages = {639--646},
title = {{Learning Programs: A Hierarchical Bayesian Approach}},
url = {http://www.icml2010.org/papers/568.pdf},
year = {2010}
}
@techreport{Reed,
abstract = {We propose the neural programmer-interpreter (NPI): a recurrent and composi-tional neural network that learns to represent and execute programs. NPI has three learnable components: a task-agnostic recurrent core, a persistent key-value program memory, and domain-specific encoders that enable a single NPI to operate in multiple perceptually diverse environments with distinct affordances. By learning to compose lower-level programs to express higher-level programs, NPI reduces sample complexity and increases generalization ability compared to sequence-to-sequence LSTMs. The program memory allows efficient learning of additional tasks by building on existing programs. NPI can also harness the environment (e.g. a scratch pad with read-write pointers) to cache intermediate results of computation , lessening the long-term memory burden on recurrent hidden units. In this work we train the NPI with fully-supervised execution traces; each program has example sequences of calls to the immediate subprograms conditioned on the input. Rather than training on a huge number of relatively weak labels, NPI learns from a small number of rich examples. We demonstrate the capability of our model to learn several types of compositional programs: addition, sorting, and canonicalizing 3D models. Furthermore, a single NPI learns to execute these programs and all 21 associated subprograms.},
archivePrefix = {arXiv},
arxivId = {1511.06279v4},
author = {Reed, Scott and {De Freitas}, Nando},
eprint = {1511.06279v4},
file = {:Users/nrindtor/Library/Application Support/Mendeley Desktop/Downloaded/Reed, De Freitas - Unknown - NEURAL PROGRAMMER-INTERPRETERS.pdf:pdf},
title = {{NEURAL PROGRAMMER-INTERPRETERS}},
url = {https://arxiv.org/pdf/1511.06279.pdf}
}
@techreport{Bosnjak2017,
abstract = {Given that in practice training data is scarce for all but a small set of problems, a core question is how to incorporate prior knowledge into a model. In this paper, we consider the case of prior procedural knowledge for neural networks, such as knowing how a program should traverse a sequence, but not what local actions should be performed at each step. To this end, we present an end-to-end differentiable interpreter for the programming language Forth which enables programmers to write program sketches with slots that can be filled with behaviour trained from program input-output data. We can optimise this behaviour directly through gradient descent techniques on user-specified objectives, and also integrate the program into any larger neural computation graph. We show empirically that our interpreter is able to effectively leverage different levels of prior program structure and learn complex behaviours such as sequence sorting and addition. When connected to outputs of an LSTM and trained jointly, our interpreter achieves state-of-the-art accuracy for end-to-end reasoning about quantities expressed in natural language stories.},
archivePrefix = {arXiv},
arxivId = {1605.06640v3},
author = {Bo{\v{s}}njak, Matko and Rockt{\"{a}}schel, Tim and Naradowsky, Jason and Riedel, Sebastian},
eprint = {1605.06640v3},
file = {:Users/nrindtor/Library/Application Support/Mendeley Desktop/Downloaded/Bo{\v{s}}njak et al. - 2017 - Programming with a Differentiable Forth Interpreter.pdf:pdf},
title = {{Programming with a Differentiable Forth Interpreter}},
url = {https://arxiv.org/pdf/1605.06640.pdf},
year = {2017}
}
@techreport{Duvenaud,
abstract = {Despite its importance, choosing the structural form of the kernel in nonparametric regression remains a black art. We define a space of kernel structures which are built compositionally by adding and multiplying a small number of base kernels. We present a method for searching over this space of structures which mirrors the scientific discovery process. The learned structures can often decompose functions into interpretable components and enable long-range extrapolation on time-series datasets. Our structure search method outperforms many widely used kernels and kernel combination methods on a variety of prediction tasks.},
author = {Duvenaud, David and Lloyd, James Robert and Grosse, Roger and Tenenbaum, Joshua B and Ghahramani, Zoubin},
file = {:Users/nrindtor/Library/Application Support/Mendeley Desktop/Downloaded/Duvenaud et al. - Unknown - Structure Discovery in Nonparametric Regression through Compositional Kernel Search.pdf:pdf},
title = {{Structure Discovery in Nonparametric Regression through Compositional Kernel Search}},
url = {http://proceedings.mlr.press/v28/duvenaud13.pdf}
}
@techreport{Lake2015,
abstract = {People learning new concepts can often generalize successfully from just a single example, yet machine learning algorithms typically require tens or hundreds of examples to perform with similar accuracy. People can also use learned concepts in richer ways than conventional algorithms-for action, imagination, and explanation. We present a computational model that captures these human learning abilities for a large class of simple visual concepts: handwritten characters from the world's alphabets. The model represents concepts as simple programs that best explain observed examples under a Bayesian criterion. On a challenging one-shot classification task, the model achieves human-level performance while outperforming recent deep learning approaches. We also present several "visual Turing tests" probing the model's creative generalization abilities, which in many cases are indistinguishable from human behavior. D espite remarkable advances in artificial intelligence and machine learning, two aspects of human conceptual knowledge have eluded machine systems. First, for most interesting kinds of natural and man-made categories, people can learn a new concept from just one or a handful of examples, whereas standard algorithms in machine learning require tens or hundreds of examples to perform similarly. For instance, people may only need to see one example of a novel two-wheeled vehicle (Fig. 1A) in order to grasp the boundaries of the new concept, and even children can make meaningful generalizations via "one-shot learning" (1-3). In contrast, many of the leading approaches in machine learning are also the most data-hungry, especially "deep learning" models that have achieved new levels of performance on object and speech recognition benchmarks (4-9). Second , people learn richer representations than machines do, even for simple concepts (Fig. 1B), using them for a wider range of functions, including (Fig. 1, ii) creating new exemplars (10), (Fig. 1, iii) parsing objects into parts and relations (11), and (Fig. 1, iv) creating new abstract categories of objects based on existing categories (12, 13). In contrast, the best machine classifiers do not perform these additional functions, which are rarely studied and usually require specialized algorithms. A central challenge is to explain these two aspects of human-level concept learning: How do people learn new concepts from just one or a few examples? And how do people learn such abstract, rich, and flexible rep-resentations? An even greater challenge arises when putting them together: How can learning succeed from such sparse data yet also produce such rich representations? For any theory of RESEARCH 1332 11 DECEMBER 2015 • VOL 350 ISSUE 6266 sciencemag.org SCIENCE Fig. 1. People can learn rich concepts from limited data. (A and B) A single example of a new concept (red boxes) can be enough information to support the (i) classification of new examples, (ii) generation of new examples, (iii) parsing an object into parts and relations (parts segmented by color), and (iv) generation of new concepts from related concepts. [Image credit for (A), iv, bottom: With permission from Glenn Roberts and Motorcycle Mojo Magazine]},
author = {Lake, Brenden M and Salakhutdinov, Ruslan and Tenenbaum, Joshua B},
file = {:Users/nrindtor/Library/Application Support/Mendeley Desktop/Downloaded/Lake, Salakhutdinov, Tenenbaum - 2015 - Human-level concept learning through probabilistic program induction.pdf:pdf},
title = {{Human-level concept learning through probabilistic program induction}},
url = {www.sciencemag.org},
year = {2015}
}
@article{Nie2017,
abstract = {Flexible estimation of heterogeneous treatment effects lies at the heart of many statistical challenges, such as personalized medicine and optimal resource allocation. In this paper, we develop a general class of two-step algorithms for heterogeneous treatment effect estimation in observational studies. We first estimate marginal effects and treatment propensities in order to form an objective function that isolates the causal component of the signal. Then, we optimize this data-adaptive objective function. Our approach has several advantages over existing methods. From a practical perspective, our method is flexible and easy to use: For both steps, we can use any loss-minimization method, e.g., penalized regression, deep neutral networks, or boosting; moreover, these methods can be fine-tuned by cross validation. Meanwhile, in the case of penalized kernel regression, we show that our method has a quasi-oracle property: Even if the pilot estimates for marginal effects and treatment propensities are not particularly accurate, we achieve the same regret bounds as an oracle who has a priori knowledge of these two nuisance components. We implement variants of our approach based on both penalized regression and boosting in a variety of simulation setups, and find promising performance relative to existing baselines.},
archivePrefix = {arXiv},
arxivId = {1712.04912},
author = {Nie, Xinkun and Wager, Stefan},
eprint = {1712.04912},
file = {:Users/nrindtor/Documents/papers{\_}pile/1712.04912.pdf:pdf},
journal = {arxiv},
keywords = {boosting,causal inference,empirical risk minimization,kernel regression,pe-},
number = {2017},
pages = {1--46},
title = {{Quasi-Oracle Estimation of Heterogeneous Treatment Effects}},
url = {http://arxiv.org/abs/1712.04912},
year = {2017}
}
@article{Riquelme2018,
abstract = {Recent advances in deep reinforcement learning have made significant strides in performance on applications such as Go and Atari games. However, developing practical methods to balance exploration and exploitation in complex domains remains largely unsolved. Thompson Sampling and its extension to reinforcement learning provide an elegant approach to exploration that only requires access to posterior samples of the model. At the same time, advances in approximate Bayesian methods have made posterior approximation for flexible neural network models practical. Thus, it is attractive to consider approximate Bayesian neural networks in a Thompson Sampling framework. To understand the impact of using an approximate posterior on Thompson Sampling, we benchmark well-established and recently developed methods for approximate posterior sampling combined with Thompson Sampling over a series of contextual bandit problems. We found that many approaches that have been successful in the supervised learning setting underperformed in the sequential decision-making scenario. In particular, we highlight the challenge of adapting slowly converging uncertainty estimates to the online setting.},
archivePrefix = {arXiv},
arxivId = {1802.09127},
author = {Riquelme, Carlos and Tucker, George and Snoek, Jasper},
eprint = {1802.09127},
file = {:Users/nrindtor/Documents/papers{\_}pile/1802.09127.pdf:pdf},
title = {{Deep Bayesian Bandits Showdown: An Empirical Comparison of Bayesian Deep Networks for Thompson Sampling}},
url = {http://arxiv.org/abs/1802.09127},
year = {2018}
}
@article{Betancourt2017,
abstract = {Hamiltonian Monte Carlo has proven a remarkable empirical success, but only recently have we begun to develop a rigorous understanding of why it performs so well on difficult problems and how it is best applied in practice. Unfortunately, that understanding is confined within the mathematics of differential geometry which has limited its dissemination, especially to the applied communities for which it is particularly important. In this review I provide a comprehensive conceptual account of these theoretical foundations, focusing on developing a principled intuition behind the method and its optimal implementations rather of any exhaustive rigor. Whether a practitioner or a statistician, the dedicated reader will acquire a solid grasp of how Hamiltonian Monte Carlo works, when it succeeds, and, perhaps most importantly, when it fails.},
annote = {typical set = the zone in high dimensional space with high prob. density and volume

concentration of measure = as dimensionality increases, the typical set decreases 

as the concentration of measure takes place, the typical set becomes the only relevant space to explore},
archivePrefix = {arXiv},
arxivId = {1701.02434},
author = {Betancourt, Michael},
doi = {10.2307/1126509},
eprint = {1701.02434},
file = {:Users/nrindtor/Library/Application Support/Mendeley Desktop/Downloaded/Betancourt - 2018 - A Conceptual Introduction to Hamiltonian Monte Carlo.pdf:pdf},
isbn = {9788578110796},
issn = {00093920},
pmid = {25246403},
title = {{A Conceptual Introduction to Hamiltonian Monte Carlo}},
url = {https://arxiv.org/pdf/1701.02434.pdf http://arxiv.org/abs/1701.02434},
year = {2017}
}
@article{Carpenter2017a,
abstract = {Stan is a probabilistic programming language for specifying statistical models. A Stan program imperatively defines a log probability function over parameters conditioned on specified data and constants. As of version 2.2.0, Stan provides full Bayesian inference for continuous-variable models through Markov chain Monte Carlo methods such as the No-U-Turn sampler, an adaptive form of Hamiltonian Monte Carlo sampling. Penalized maximum likelihood estimates are calculated using optimization methods such as the Broyden-Fletcher-Goldfarb-Shanno algorithm. Stan is also a platform for computing log densities and their gradients and Hessians, which can be used in alternative algorithms such as variational Bayes, expectation propa- gation, and marginal inference using approximate integration. To this end, Stan is set up so that the densities, gradients, and Hessians, along with intermediate quantities of the algorithm such as acceptance probabilities, are easily accessible. Stan can be called from the command line, through R using the RStan package, or through Python using the PyStan package. All three interfaces support sampling or optimization-based inference and analysis, and RStan and PyStan also provide access to log probabilities, gradients, Hessians, and data I/O.},
archivePrefix = {arXiv},
arxivId = {1210.1088},
author = {Carpenter, Bob and Guo, Jiqiang and Hoffman, Matthew D. and Brubaker, Marcus and Gelman, Andrew and Lee, Daniel and Goodrich, Ben and Li, Peter and Riddell, Allen and Betancourt, Michael},
doi = {10.18637/jss.v076.i01},
eprint = {1210.1088},
file = {:Users/nrindtor/Documents/papers{\_}pile/v76i01.pdf:pdf},
isbn = {0394-6320 (Print) 0394-6320 (Linking)},
issn = {1548-7660},
journal = {Journal of Statistical Software},
number = {1},
pmid = {19505410},
title = {{Stan : A Probabilistic Programming Language }},
volume = {76},
year = {2017}
}
@techreport{Pfeffer2009,
abstract = {Abstract We introduce an object - oriented paradigm for probabilistic programming , embodied in the Figaro language . Models in Figaro are objects, and may have properties such as conditions, constraints and relationships to other objects. Figaro model classes are ...},
author = {Pfeffer, Avi},
booktitle = {Charles River Analytics Technical Report},
file = {:Users/nrindtor/Library/Application Support/Mendeley Desktop/Downloaded/Pfeffer, Analytics - Unknown - Figaro An Object-Oriented Probabilistic Programming Language.pdf:pdf},
isbn = {9781577354260},
issn = {10450823},
pages = {1--9},
title = {{Figaro: An object-oriented probabilistic programming language}},
url = {https://www.cs.tufts.edu/{~}nr/cs257/archive/avi-pfeffer/figaro.pdf http://www.cs.tufts.edu/{~}nr/cs257/archive/avi-pfeffer/figaro.pdf{\%}5Cnpapers2://publication/uuid/0E83E526-451F-41EA-ACBE-7150FF7584D4},
year = {2009}
}
@techreport{Hoffman2014,
abstract = {Hamiltonian Monte Carlo (HMC) is a Markov chain Monte Carlo (MCMC) algorithm that avoids the random walk behavior and sensitivity to correlated parameters that plague many MCMC methods by taking a series of steps informed by first-order gradient information. These features allow it to converge to high-dimensional target distributions much more quickly than simpler methods such as random walk Metropolis or Gibbs sampling. However, HMC's performance is highly sensitive to two user-specified parameters: a step size and a desired number of steps L. In particular, if L is too small then the algorithm exhibits undesirable random walk behavior, while if L is too large the algorithm wastes computation. We introduce the No-U-Turn Sampler (NUTS), an extension to HMC that eliminates the need to set a number of steps L. NUTS uses a recursive algorithm to build a set of likely candidate points that spans a wide swath of the target distribution, stopping automatically when it starts to double back and retrace its steps. Empirically, NUTS performs at least as efficiently as (and sometimes more efficiently than) a well tuned standard HMC method, without requiring user intervention or costly tuning runs. We also derive a method for adapting the step size parameter on the fly based on primal-dual averaging. NUTS can thus be used with no hand-tuning at all, making it suitable for applications such as BUGS-style automatic inference engines that require efficient "turnkey" samplers.},
author = {Hoffman, Matthew D and Gelman, Andrew},
booktitle = {Journal of Machine Learning Research},
file = {:Users/nrindtor/Library/Application Support/Mendeley Desktop/Downloaded/Hoffman, Gelman - 2014 - The No-U-Turn Sampler Adaptively Setting Path Lengths in Hamiltonian Monte Carlo.pdf:pdf},
keywords = {Bayesian inference,Hamiltonian Monte Carlo,Markov chain Monte Carlo,adaptive Monte Carlo,dual averaging},
pages = {1593--1623},
title = {{The No-U-Turn Sampler: Adaptively Setting Path Lengths in Hamiltonian Monte Carlo}},
url = {http://mcmc-jags.sourceforge.net},
volume = {15},
year = {2014}
}
@techreport{Brooks2012,
abstract = {Hamiltonian dynamics can be used to produce distant proposals for the Metropolis algorithm, thereby avoiding the slow exploration of the state space that results from the diffusive behaviour of simple random-walk proposals. Though originating in physics, Hamiltonian dynamics can be applied to most problems with continuous state spaces by simply introducing fictitious "momentum" variables. A key to its usefulness is that Hamiltonian dynamics preserves volume, and its trajectories can thus be used to define complex mappings without the need to account for a hard-to-compute Jacobian factor-a property that can be exactly maintained even when the dynamics is approximated by discretizing time. In this review, I discuss theoretical and practical aspects of Hamiltonian Monte Carlo, and present some of its variations, including using windows of states for deciding on acceptance or rejection, computing trajectories using fast approximations, tempering during the course of a trajectory to handle isolated modes, and shortcut methods that prevent useless trajectories from taking much computation time.},
archivePrefix = {arXiv},
arxivId = {1206.1901v1},
author = {Brooks, Steve and Gelman, Andrew and Jones, Galin and Meng, Xiao-Li and Neal, Radford M},
eprint = {1206.1901v1},
file = {:Users/nrindtor/Documents/papers{\_}pile/1206.1901.pdf:pdf},
keywords = {()},
title = {{MCMC using Hamiltonian dynamics}},
url = {https://arxiv.org/pdf/1206.1901.pdf},
year = {2012}
}
@article{Shalit2016,
abstract = {There is intense interest in applying machine learning to problems of causal inference in fields such as healthcare, economics and education. In particular, individual-level causal inference has important applications such as precision medicine. We give a new theoretical analysis and family of algorithms for predicting individual treatment effect (ITE) from observational data, under the assumption known as strong ignorability. The algorithms learn a "balanced" representation such that the induced treated and control distributions look similar. We give a novel, simple and intuitive generalization-error bound showing that the expected ITE estimation error of a representation is bounded by a sum of the standard generalization-error of that representation and the distance between the treated and control distributions induced by the representation. We use Integral Probability Metrics to measure distances between distributions, deriving explicit bounds for the Wasserstein and Maximum Mean Discrepancy (MMD) distances. Experiments on real and simulated data show the new algorithms match or outperform the state-of-the-art.},
archivePrefix = {arXiv},
arxivId = {1606.03976},
author = {Shalit, Uri and Johansson, Fredrik D. and Sontag, David},
doi = {10.1146/annurev-pathol-011811-132457},
eprint = {1606.03976},
file = {:Users/nrindtor/Library/Application Support/Mendeley Desktop/Downloaded/Shalit, Johansson, Sontag - 2016 - Estimating individual treatment effect generalization bounds and algorithms.pdf:pdf},
isbn = {9781424427109},
issn = {0272-3638},
month = {jun},
pmid = {27862035},
title = {{Estimating individual treatment effect: generalization bounds and algorithms}},
url = {https://arxiv.org/abs/1606.03976 http://arxiv.org/abs/1606.03976},
year = {2016}
}
@article{Woodcock2017,
abstract = {This review considers master protocols, which involve the study of one or more interventions in multiple diseases or of a single disease with multiple interventions.},
author = {Woodcock, Janet and LaVange, Lisa M},
doi = {10.1056/NEJMra1510062},
file = {:Users/nrindtor/Documents/papers{\_}pile/woodcock2017.pdf:pdf},
isbn = {0028-4793},
issn = {1533-4406},
journal = {The New England journal of medicine},
number = {1},
pages = {62--70},
pmid = {28679092},
title = {{Master Protocols to Study Multiple Therapies, Multiple Diseases, or Both.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/28679092},
volume = {377},
year = {2017}
}
@article{Massard2017,
abstract = {High-throughput genomic analyses may improve outcomes in patients with advanced cancers. MOSCATO 01 is a prospective clinical trial evaluating the clinical benefit of this approach. Nucleic acids were extracted from fresh-frozen tumor biopsies and analyzed by array comparative genomic hybridization, next-generation sequencing, and RNA sequencing. The primary objective was to evaluate clinical benefit as measured by the percentage of patients presenting progression-free survival (PFS) on matched therapy (PFS2) 1.3-fold longer than the PFS on prior therapy (PFS1). A total of 1,035 adult patients were included, and a biopsy was performed in 948. An actionable molecular alteration was identified in 411 of 843 patients with a molecular portrait. A total of 199 patients were treated with a targeted therapy matched to a genomic alteration. The PFS2/PFS1 ratio was {\textgreater}1.3 in 33{\%} of the patients (63/193). Objective responses were observed in 22 of 194 patients (11{\%}; 95{\%} CI, 7{\%}-17{\%}), and median overall survival was 11.9 months (95{\%} CI, 9.5-14.3 months).SIGNIFICANCE: This study suggests that high-throughput genomics could improve outcomes in a subset of patients with hard-to-treat cancers. Although these results are encouraging, only 7{\%} of the successfully screened patients benefited from this approach. Randomized trials are needed to validate this hypothesis and to quantify the magnitude of benefit. Expanding drug access could increase the percentage of patients who benefit. Cancer Discov; 7(6); 1-10. {\textcopyright}2017 AACR.},
author = {Massard, Christophe and Michiels, Stefan and Fert{\'{e}}, Charles and {Le Deley}, Marie C{\'{e}}cile and Lacroix, Ludovic and Hollebecque, Antoine and Verlingue, Loic and Ileana, Ecaterina and Rosellini, Silvia and Ammari, Samy and Ngo-Camus, Maud and Bahleda, Rastislav and Gazzah, Anas and Varga, Andrea and Postel-Vinay, Sophie and Loriot, Yohann and Even, Caroline and Breuskin, Ingrid and Auger, Nathalie and Job, Bastien and {De Baere}, Thierry and Deschamps, Frederic and Vielh, Philippe and Scoazec, Jean Yves and Lazar, Vladimir and Richon, Catherine and Ribrag, Vincent and Deutsch, Eric and Angevin, Eric and Vassal, Gilles and Eggermont, Alexander and Andr{\'{e}}, Fabrice and Soria, Jean Charles},
doi = {10.1158/2159-8290.CD-16-1396},
file = {:Users/nrindtor/Library/Application Support/Mendeley Desktop/Downloaded/Massard et al. - 2017 - High-throughput genomics and clinical outcome in hard-to-treat advanced cancers Results of the MOSCATO 01 trial.pdf:pdf},
isbn = {2159-8290 (Electronic)$\backslash$r2159-8274 (Linking)},
issn = {21598290},
journal = {Cancer Discovery},
number = {6},
pages = {586--595},
pmid = {28365644},
title = {{High-throughput genomics and clinical outcome in hard-to-treat advanced cancers: Results of the MOSCATO 01 trial}},
volume = {7},
year = {2017}
}
@article{Blumenthal2010,
abstract = {In any field, improving performance and accountability depends on having a shared goal that unites the interests and activities of all stakeholders. In health care, however, stakeholders have myriad, often conflicting goals, including access to services, profitability, high quality, cost containment, safety, convenience, patient-centeredness, and satisfaction. Lack of clarity about goals has led to divergent approaches, gaming of the system, and slow progress in performance improvement. Achieving high value for patients must become the overarching goal of health care delivery, with value defined as the health outcomes achieved per dollar spent.(1) This goal is what matters for patients and unites . . .},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Blumenthal, David and Tavenner, Marilyn},
doi = {10.1056/NEJMp1002530},
eprint = {arXiv:1011.1669v3},
file = {:Users/nrindtor/Documents/papers{\_}pile/Collins and Varmus 2015.pdf:pdf},
isbn = {0028-4793},
issn = {15334406},
journal = {The New England journal of medicine},
number = {1},
pages = {1--3},
pmid = {20573919},
title = {{A New Initiative on Precision Medicine}},
url = {http://scholar.google.com/scholar?hl=en{\&}btnG=Search{\&}q=intitle:New+engla+nd+journal{\#}0},
volume = {363},
year = {2010}
}
@article{Chernozhukov2016,
abstract = {Most modern supervised statistical/machine learning (ML) methods are explicitly designed to solve prediction problems very well. Achieving this goal does not imply that these methods automatically deliver good estimators of causal parameters. Examples of such parameters include individual regression coefficients, average treatment effects, average lifts, and demand or supply elasticities. In fact, estimates of such causal parameters obtained via naively plugging ML estimators into estimating equations for such parameters can behave very poorly due to the regularization bias. Fortunately, this regularization bias can be removed by solving auxiliary prediction problems via ML tools. Specifically, we can form an orthogonal score for the target low-dimensional parameter by combining auxiliary and main ML predictions. The score is then used to build a de-biased estimator of the target parameter which typically will converge at the fastest possible 1/root(n) rate and be approximately unbiased and normal, and from which valid confidence intervals for these parameters of interest may be constructed. The resulting method thus could be called a "double ML" method because it relies on estimating primary and auxiliary predictive models. In order to avoid overfitting, our construction also makes use of the K-fold sample splitting, which we call cross-fitting. This allows us to use a very broad set of ML predictive methods in solving the auxiliary and main prediction problems, such as random forest, lasso, ridge, deep neural nets, boosted trees, as well as various hybrids and aggregators of these methods.},
archivePrefix = {arXiv},
arxivId = {1608.00060},
author = {Chernozhukov, Victor and Chetverikov, Denis and Demirer, Mert and Duflo, Esther and Hansen, Christian and Newey, Whitney and Robins, James},
doi = {10.1920/wp.cem.2016.4916},
eprint = {1608.00060},
file = {:Users/nrindtor/Documents/papers{\_}pile/DML7{\_}16.pdf:pdf},
isbn = {1608.00060v5},
issn = {0002-8282},
journal = {arxiv},
keywords = {boosted trees,cross-fit,debiased machine learning,deep learning,double machine learning,efficiency,efficient score,lasso,nal score,neural nets,neyman,optimality,orthogo-,orthogonalization,post-machine-learning and post-regularization infe,random forest},
number = {Ml},
pages = {1--30},
pmid = {20004811},
title = {{Double/Debiased Machine Learning for Treatment and Causal Parameters}},
url = {http://arxiv.org/abs/1608.00060},
year = {2016}
}
@book{Pearl2013,
author = {Pearl, Judea},
booktitle = {Causality},
doi = {10.1017/cbo9780511803161.005},
file = {:Users/nrindtor/Documents/papers{\_}pile/causal{\_}diagrams{\_}and{\_}the{\_}identification{\_}of{\_}causal{\_}effects.pdf:pdf},
isbn = {9780511803161},
pages = {65--106},
title = {{Causal Diagrams and the Identification of Causal Effects}},
year = {2013}
}
@article{Mining2014,
author = {Mining, Data},
doi = {10.4172/2153-0602.1000},
file = {:Users/nrindtor/Documents/papers{\_}pile/causal-inference-in-the-age-of-decision-medicine-2153-0602.1000163.pdf:pdf},
keywords = {assignment mechanism,coronary heart disease,structural equation modeling},
number = {1},
pages = {1--6},
title = {{Data Mining in Genomics {\&} Proteomics Causal Inference in the Age of Decision Medicine}},
volume = {6},
year = {2014}
}
@article{Louizos2017a,
abstract = {Learning individual-level causal effects from observational data, such as inferring the most effective medication for a specific patient, is a problem of growing importance for policy makers. The most important aspect of inferring causal effects from observational data is the handling of confounders, factors that affect both an intervention and its outcome. A carefully designed observational study attempts to measure all important confounders. However, even if one does not have direct access to all confounders, there may exist noisy and uncertain measurement of proxies for confounders. We build on recent advances in latent variable modeling to simultaneously estimate the unknown latent space summarizing the confounders and the causal effect. Our method is based on Variational Autoencoders (VAE) which follow the causal structure of inference with proxies. We show our method is significantly more robust than existing methods, and matches the state-of-the-art on previous benchmarks focused on individual treatment effects.},
archivePrefix = {arXiv},
arxivId = {1705.08821},
author = {Louizos, Christos and Shalit, Uri and Mooij, Joris and Sontag, David and Zemel, Richard and Welling, Max},
eprint = {1705.08821},
file = {:Users/nrindtor/Documents/papers{\_}pile/1705.08821.pdf:pdf},
issn = {10495258},
number = {Nips},
title = {{Causal Effect Inference with Deep Latent-Variable Models}},
url = {http://arxiv.org/abs/1705.08821},
year = {2017}
}
